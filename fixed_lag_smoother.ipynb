{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fixed_lag_smoother.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterchang0414/hmm-jax/blob/main/fixed_lag_smoother.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flax"
      ],
      "metadata": {
        "id": "vR05NbgRYED1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a59373-3824-4b78-b7b8-e66d010f14a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 184 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 12.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 72 kB 604 kB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference and learning code for Hidden Markov Models using discrete observations.\n",
        "# Has Jax version of each function. For the Numpy version, please see hmm_numpy_lib.py\n",
        "# The Jax version of inference (not learning)\n",
        "# has been upstreamed to https://github.com/deepmind/distrax/blob/master/distrax/_src/utils/hmm.py.\n",
        "# This version is kept for historical purposes.\n",
        "# Author: Gerardo Duran-Martin (@gerdm), Aleyna Kara (@karalleyna), Kevin Murphy (@murphyk)\n",
        "\n",
        "from jax import lax\n",
        "from jax.scipy.special import logit\n",
        "from functools import partial\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from scipy.special import softmax\n",
        "from jax import vmap\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import jax\n",
        "import itertools\n",
        "from jax import jit\n",
        "from jax.nn import softmax\n",
        "from jax.random import PRNGKey, split, normal\n",
        "from jax.random import split, randint, PRNGKey, normal, permutation\n",
        "\n",
        "import flax\n",
        "\n",
        "'''\n",
        "Hidden Markov Model class used in jax implementations of inference algorithms.\n",
        "The functions of optimizers expect that the type of its parameters \n",
        "is pytree. So, they cannot work on a vanilla dataclass. To see more:\n",
        "                https://github.com/google/jax/issues/2371\n",
        "Since the flax.dataclass is registered pytree beforehand, it facilitates to use\n",
        "jit, vmap and optimizers on the hidden markov model.\n",
        "'''\n",
        "\n",
        "\n",
        "@flax.struct.dataclass\n",
        "class HMMJax:\n",
        "    trans_mat: jnp.array  # A : (n_states, n_states)\n",
        "    obs_mat: jnp.array  # B : (n_states, n_obs)\n",
        "    init_dist: jnp.array  # pi : (n_states)\n",
        "\n",
        "\n",
        "def normalize(u, axis=0, eps=1e-15):\n",
        "    '''\n",
        "    Normalizes the values within the axis in a way that they sum up to 1.\n",
        "    Parameters\n",
        "    ----------\n",
        "    u : array\n",
        "    axis : int\n",
        "    eps : float\n",
        "        Threshold for the alpha values\n",
        "    Returns\n",
        "    -------\n",
        "    * array\n",
        "        Normalized version of the given matrix\n",
        "    * array(seq_len, n_hidden) :\n",
        "        The values of the normalizer\n",
        "    '''\n",
        "    u = jnp.where(u == 0, 0, jnp.where(u < eps, eps, u))\n",
        "    c = u.sum(axis=axis, keepdims=True)\n",
        "    c = jnp.where(c == 0, 1, c)\n",
        "    return u / c, c\n",
        "\n",
        "\n",
        "##############################\n",
        "# Inference\n",
        "\n",
        "def hmm_forwards_jax(params, obs_seq, length=None):\n",
        "    '''\n",
        "    Calculates a belief state\n",
        "    Parameters\n",
        "    ----------\n",
        "    params : HMMJax\n",
        "        Hidden Markov Model\n",
        "    obs_seq: array(seq_len)\n",
        "        History of observable events\n",
        "    Returns\n",
        "    -------\n",
        "    * float\n",
        "        The loglikelihood giving log(p(x|model))\n",
        "    * array(seq_len, n_hidden) :\n",
        "        All alpha values found for each sample\n",
        "    '''\n",
        "    seq_len = len(obs_seq)\n",
        "\n",
        "    if length is None:\n",
        "        length = seq_len\n",
        "\n",
        "    trans_mat, obs_mat, init_dist = params.trans_mat, params.obs_mat, params.init_dist\n",
        "\n",
        "    trans_mat = jnp.array(trans_mat)\n",
        "    obs_mat = jnp.array(obs_mat)\n",
        "    init_dist = jnp.array(init_dist)\n",
        "\n",
        "    n_states, n_obs = obs_mat.shape\n",
        "\n",
        "    def scan_fn(carry, t):\n",
        "        (alpha_prev, log_ll_prev) = carry\n",
        "        alpha_n = jnp.where(t < length,\n",
        "                            obs_mat[:, obs_seq[t]] * (alpha_prev[:, None] * trans_mat).sum(axis=0),\n",
        "                            jnp.zeros_like(alpha_prev))\n",
        "\n",
        "        alpha_n, cn = normalize(alpha_n)\n",
        "        carry = (alpha_n, jnp.log(cn) + log_ll_prev)\n",
        "\n",
        "        return carry, alpha_n\n",
        "\n",
        "    # initial belief state\n",
        "    alpha_0, c0 = normalize(init_dist * obs_mat[:, obs_seq[0]])\n",
        "\n",
        "    # setup scan loop\n",
        "    init_state = (alpha_0, jnp.log(c0))\n",
        "    ts = jnp.arange(1, seq_len)\n",
        "    carry, alpha_hist = lax.scan(scan_fn, init_state, ts)\n",
        "\n",
        "    # post-process\n",
        "    alpha_hist = jnp.vstack([alpha_0.reshape(1, n_states), alpha_hist])\n",
        "    (alpha_final, log_ll) = carry\n",
        "    return log_ll, alpha_hist\n",
        "\n",
        "\n",
        "@jit\n",
        "def hmm_loglikelihood_jax(params, observations, lens):\n",
        "    '''\n",
        "    Finds the loglikelihood of each observation sequence parallel using vmap.\n",
        "    Parameters\n",
        "    ----------\n",
        "    params : HMMJax\n",
        "        Hidden Markov Model\n",
        "    observations: array(N, seq_len)\n",
        "        Batch of observation sequences\n",
        "    lens : array(N, seq_len)\n",
        "        Consists of the valid length of each observation sequence\n",
        "    Returns\n",
        "    -------\n",
        "    * array(N, seq_len)\n",
        "        Consists of the loglikelihood of each observation sequence\n",
        "    '''\n",
        "\n",
        "    def forward_(params, x, length):\n",
        "        return hmm_forwards_jax(params, x, length)[0]\n",
        "\n",
        "    return vmap(forward_, in_axes=(None, 0, 0))(params, observations, lens)\n",
        "\n",
        "\n",
        "def hmm_backwards_jax(params, obs_seq, length=None):\n",
        "    '''\n",
        "    Computes the backwards probabilities\n",
        "    Parameters\n",
        "    ----------\n",
        "    params : HMMJax\n",
        "        Hidden Markov Model\n",
        "    obs_seq: array(seq_len,)\n",
        "        History of observable events\n",
        "    length : array(seq_len,)\n",
        "        The valid length of the observation sequence\n",
        "    Returns\n",
        "    -------\n",
        "    * array(seq_len, n_states)\n",
        "       Beta values\n",
        "    '''\n",
        "    seq_len = len(obs_seq)\n",
        "\n",
        "    if length is None:\n",
        "        length = seq_len\n",
        "\n",
        "    trans_mat, obs_mat, init_dist = params.trans_mat, params.obs_mat, params.init_dist\n",
        "\n",
        "    trans_mat = jnp.array(trans_mat)\n",
        "    obs_mat = jnp.array(obs_mat)\n",
        "    init_dist = jnp.array(init_dist)\n",
        "\n",
        "    n_states, n_obs = obs_mat.shape\n",
        "\n",
        "    beta_t = jnp.ones((n_states,))\n",
        "\n",
        "    def scan_fn(beta_prev, t):\n",
        "        beta_t = jnp.where(t > length,\n",
        "                           jnp.zeros_like(beta_prev),\n",
        "                           normalize((beta_prev * obs_mat[:, obs_seq[-t + 1]] * trans_mat).sum(axis=1))[0])\n",
        "        return beta_t, beta_t\n",
        "\n",
        "    ts = jnp.arange(2, seq_len + 1)\n",
        "    _, beta_hist = lax.scan(scan_fn, beta_t, ts)\n",
        "\n",
        "    beta_hist = jnp.flip(jnp.vstack([beta_t.reshape(1, n_states), beta_hist]), axis=0)\n",
        "\n",
        "    return beta_hist\n",
        "\n",
        "\n",
        "def hmm_forwards_backwards_jax(params, obs_seq, length=None):\n",
        "    '''\n",
        "    Computes, for each time step, the marginal conditional probability that the Hidden Markov Model was\n",
        "    in each possible state given the observations that were made at each time step, i.e.\n",
        "    P(z[i] | x[0], ..., x[num_steps - 1]) for all i from 0 to num_steps - 1\n",
        "    Parameters\n",
        "    ----------\n",
        "    params : HMMJax\n",
        "        Hidden Markov Model\n",
        "    obs_seq: array(seq_len)\n",
        "        History of observed states\n",
        "    Returns\n",
        "    -------\n",
        "    * array(seq_len, n_states)\n",
        "        Alpha values\n",
        "    * array(seq_len, n_states)\n",
        "        Beta values\n",
        "    * array(seq_len, n_states)\n",
        "        Marginal conditional probability\n",
        "    * float\n",
        "        The loglikelihood giving log(p(x|model))\n",
        "    '''\n",
        "    seq_len = len(obs_seq)\n",
        "    if length is None:\n",
        "        length = seq_len\n",
        "\n",
        "    def gamma_t(t):\n",
        "        gamma_t = jnp.where(t < length,\n",
        "                            alpha[t] * beta[t - length],\n",
        "                            jnp.zeros((n_states,)))\n",
        "        return gamma_t\n",
        "\n",
        "    ll, alpha = hmm_forwards_jax(params, obs_seq, length)\n",
        "    n_states = alpha.shape[1]\n",
        "\n",
        "    beta = hmm_backwards_jax(params, obs_seq, length)\n",
        "\n",
        "    ts = jnp.arange(seq_len)\n",
        "    gamma = vmap(gamma_t, (0))(ts)\n",
        "    # gamma = alpha * jnp.roll(beta, -seq_len + length, axis=0) #: Alternative\n",
        "    gamma = vmap(lambda x: normalize(x)[0])(gamma)\n",
        "    return alpha, beta, gamma, ll\n"
      ],
      "metadata": {
        "id": "1qLvsJy1X_TM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are my contributions\n"
      ],
      "metadata": {
        "id": "bqQdk3pUzh-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@flax.struct.dataclass\n",
        "class HMMWithActionJax:\n",
        "    trans_mat: jnp.array  # A : (n_actions, n_states, n_states)\n",
        "    obs_mat: jnp.array  # B : (n_states, n_obs)\n",
        "    init_dist: jnp.array  # pi : (n_states)"
      ],
      "metadata": {
        "id": "j-ePHPODWbgo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "QDJ7YcaZGSTI"
      },
      "outputs": [],
      "source": [
        "# Naive (un-vectorized) version\n",
        "@partial(jax.jit, static_argnums=(1))\n",
        "def fixed_lag_smoother(params, win_len, alpha_win, obs_seq_win, obs, act=None):\n",
        "    '''\n",
        "    Description...\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    params      : HMMWithActionJax\n",
        "        Hidden Markov Model (with action-dependent transition)\n",
        "    win_len     : int\n",
        "        Desired window length (>= 2)\n",
        "    alpha_win   : array\n",
        "        Alpha values for the most recent win_len steps, excluding current step\n",
        "    obs_seq_win : array\n",
        "        Observations for the most recent win_len steps, excluding current step\n",
        "    obs         : int\n",
        "        New observation for the current step\n",
        "    act         : array\n",
        "        (optional) Actions for the most recent win_len steps, including current step\n",
        "    Returns\n",
        "    -------\n",
        "    * array(win_len, n_states)\n",
        "        Updated alpha values\n",
        "    * array(win_len)\n",
        "        Updated observations for the past d steps\n",
        "    * array(win_len, n_states)\n",
        "        Smoothed posteriors for the past d steps\n",
        "    * float\n",
        "        The loglikelihood of the past d steps\n",
        "    '''\n",
        "    if len(alpha_win.shape) < 2:\n",
        "        alpha_win = jnp.expand_dims(alpha_win, axis=0)\n",
        "    curr_len = alpha_win.shape[0]\n",
        "    win_len = min(win_len, curr_len+1)\n",
        "    assert win_len >= 2, \"Must keep a window of length at least 2.\"\n",
        "\n",
        "    trans_mat, obs_mat = params.trans_mat, params.obs_mat\n",
        "    n_states, n_obs = obs_mat.shape\n",
        "    \n",
        "    # If trans_mat is independent of action, adjust shape\n",
        "    if len(trans_mat.shape) < 3:\n",
        "        trans_mat = jnp.expand_dims(trans_mat, axis=0)\n",
        "        act = None\n",
        "    if act is None:\n",
        "        act = jnp.zeros(shape=(curr_len+1,), dtype=jnp.int8)\n",
        "\n",
        "    # Shift window forward by 1\n",
        "    if curr_len == win_len:\n",
        "        alpha_win = alpha_win[1:]\n",
        "        obs_seq_win = obs_seq_win[1:]\n",
        "    new_alpha, _ = normalize(\n",
        "        obs_mat[:, obs] * (alpha_win[-1][:, None] * trans_mat[act[-1]]).sum(axis=0)\n",
        "    )\n",
        "    alpha_win = jnp.concatenate((alpha_win, new_alpha[None, :]), axis=0)\n",
        "    obs_seq_win = jnp.append(obs_seq_win, obs)\n",
        "\n",
        "    # Smooth backwards inside the window\n",
        "    beta_win = jnp.ones(shape=(win_len, n_states))\n",
        "    gamma_win = jnp.array(alpha_win)\n",
        "    for t in range(win_len-2, -1, -1):\n",
        "        new_beta, _ = normalize(\n",
        "            (jnp.multiply(beta_win[t+1,:], obs_mat[:, obs_seq_win[t+1]]) *\n",
        "             trans_mat[act[t]]).sum(axis=1)\n",
        "        )\n",
        "        beta_win = beta_win.at[t, :].set(new_beta)\n",
        "\n",
        "        new_gamma, _ = normalize(\n",
        "            jnp.multiply(alpha_win[t, :], beta_win[t, :])\n",
        "        )\n",
        "        gamma_win = gamma_win.at[t, :].set(new_gamma)\n",
        "    # print(alpha_win, '\\n')\n",
        "    # print(beta_win, '\\n')\n",
        "    return alpha_win, obs_seq_win, beta_win, gamma_win"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorized version\n",
        "def fixed_lag_smoother_vectorized(params, win_len, alpha_win, beta_win, obs_seq_win, obs, act=None):\n",
        "    '''\n",
        "    Description...\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    params      : HMMWithActionJax\n",
        "        Hidden Markov Model (with action-dependent transition)\n",
        "    win_len     : int\n",
        "        Desired window length (>= 2)\n",
        "    alpha_win   : array\n",
        "        Alpha values for the most recent win_len steps, excluding current step\n",
        "    beta_win   : array\n",
        "        Beta values for the most recent win_len steps, excluding current step\n",
        "    obs_seq_win : array\n",
        "        Observations for the most recent win_len steps, excluding current step\n",
        "    obs         : int\n",
        "        New observation for the current step\n",
        "    act         : array\n",
        "        (optional) Actions for the most recent win_len steps, including current step\n",
        "    Returns\n",
        "    -------\n",
        "    * array(win_len, n_states)\n",
        "        Updated alpha values\n",
        "    * array(win_len)\n",
        "        Updated observations for the past d steps\n",
        "    * array(win_len, n_states)\n",
        "        Smoothed posteriors for the past d steps\n",
        "    * float\n",
        "        The loglikelihood of the past d steps\n",
        "    '''\n",
        "    if len(alpha_win.shape) < 2:\n",
        "        alpha_win = jnp.expand_dims(alpha_win, axis=0)\n",
        "    curr_len = alpha_win.shape[0]\n",
        "    win_len = min(win_len, curr_len+1)\n",
        "    assert win_len >= 2, \"Must keep a window of length at least 2.\"\n",
        "\n",
        "    trans_mat, obs_mat = params.trans_mat, params.obs_mat\n",
        "    n_states, n_obs = obs_mat.shape\n",
        "    \n",
        "    # If trans_mat is independent of action, adjust shape\n",
        "    if len(trans_mat.shape) < 3:\n",
        "        trans_mat = jnp.expand_dims(trans_mat, axis=0)\n",
        "        act = None\n",
        "    if act is None:\n",
        "        act = jnp.zeros(shape=(curr_len+1,), dtype=jnp.int8)\n",
        "\n",
        "    # Shift window forward by 1\n",
        "    if curr_len == win_len:\n",
        "        alpha_win = alpha_win[1:]\n",
        "        obs_seq_win = obs_seq_win[1:]\n",
        "    new_alpha, _ = normalize(\n",
        "        obs_mat[:, obs] * (alpha_win[-1][:, None] * trans_mat[act[-1]]).sum(axis=0)\n",
        "    )\n",
        "    alpha_win = jnp.concatenate((alpha_win, new_alpha[None, :]), axis=0)\n",
        "    obs_seq_win = jnp.append(obs_seq_win, obs)\n",
        "\n",
        "    # Smooth backwards inside the window\n",
        "    beta_win = jnp.ones(shape=(win_len, n_states))\n",
        "    gamma_win = jnp.array(alpha_win)\n",
        "    for t in range(win_len-2, -1, -1):\n",
        "        new_beta, _ = normalize(\n",
        "            (jnp.multiply(beta_win[t+1,:], obs_mat[:, obs_seq_win[t+1]]) *\n",
        "             trans_mat[act[t]]).sum(axis=1)\n",
        "        )\n",
        "        beta_win = beta_win.at[t, :].set(new_beta)\n",
        "\n",
        "        new_gamma, _ = normalize(\n",
        "            jnp.multiply(alpha_win[t, :], beta_win[t, :])\n",
        "        )\n",
        "        gamma_win = gamma_win.at[t, :].set(new_gamma)\n",
        "    return alpha_win, obs_seq_win, beta_win, gamma_win"
      ],
      "metadata": {
        "id": "6YFXqHP00UmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Umbrella-rain toy example (AI, A Modern Approach)\n",
        "data = jnp.array([0, 0, 1, 1, 1])\n",
        "transmat = jnp.array([[0.7, 0.3], [0.3, 0.7]])\n",
        "obsmat = jnp.array([[0.9, 0.1], [0.2, 0.8]])\n",
        "prior = jnp.array([0.5, 0.5])\n",
        "\n",
        "hmm = HMMJax(trans_mat=transmat, obs_mat=obsmat, init_dist=prior)"
      ],
      "metadata": {
        "id": "VfXolxcdoyZW"
      },
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Larger-scale example for time experiments\n",
        "key = PRNGKey(0)\n",
        "data = jax.random.choice(key, 2, shape=(10000,))"
      ],
      "metadata": {
        "id": "-XgqOd8FpK07"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fixed_lag_smoother_result(params, win_len, data, prior, act=None):\n",
        "    assert data.size > 2, \"Complete observation set must be of size at least 2\"\n",
        "    alpha, _ = normalize(jnp.multiply(prior, obsmat[:, data[0]]))\n",
        "    obs_seq = jnp.array([data[0]])\n",
        "    for obs in data[1:]:\n",
        "        alpha, obs_seq, beta, gamma = fixed_lag_smoother(hmm, win_len, alpha, obs_seq, obs)\n",
        "    return alpha, beta, gamma"
      ],
      "metadata": {
        "id": "PIFlEVelxbIt"
      },
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha1, beta1, gamma1 = get_fixed_lag_smoother_result(hmm, 5, data, prior);\n",
        "print(beta1)"
      ],
      "metadata": {
        "id": "MzNB1AQO4rq6",
        "outputId": "2e8385e9-6a72-44b8-ea31-2097462156c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.57458866 0.4254113 ]\n",
            " [0.32730448 0.6726955 ]\n",
            " [0.36952144 0.63047856]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha1, beta1, gamma1 = get_fixed_lag_smoother_result(hmm, 5, data, prior);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vetEUd0WgOfS",
        "outputId": "ae16410d-edc0-42d1-c291-ac7f9db91d3f"
      },
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.35911217 0.64088786]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.36098886 0.63901114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.5864123  0.41358766]\n",
            " [0.35911217 0.64088786]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.35756814 0.64243186]\n",
            " [0.57354707 0.42645293]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.57458866 0.42541134]\n",
            " [0.32730445 0.6726955 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.3261829  0.6738171 ]\n",
            " [0.35911217 0.64088786]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.36098886 0.63901114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.59231764 0.4076824 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37749612 0.6225039 ]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6585519  0.34144813]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6585519  0.34144813]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6563695  0.3436305 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64649886 0.35350114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.5864123  0.41358766]\n",
            " [0.35911217 0.64088786]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.35756814 0.64243186]\n",
            " [0.57354707 0.42645293]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.57458866 0.42541134]\n",
            " [0.32730445 0.6726955 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32804754 0.6719524 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37749612 0.6225039 ]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6563695  0.3436305 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64649886 0.35350114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.5864123  0.41358766]\n",
            " [0.35911217 0.64088786]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.36098886 0.63901114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.59231764 0.4076824 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37397137 0.6260286 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64649886 0.35350114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.59231764 0.4076824 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37749612 0.6225039 ]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6563695  0.3436305 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64649886 0.35350114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.5864123  0.41358766]\n",
            " [0.35911217 0.64088786]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.36098886 0.63901114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.59231764 0.4076824 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37397137 0.6260286 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64327985 0.35672024]\n",
            " [0.57354707 0.42645293]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.5727641  0.4272359 ]\n",
            " [0.32267347 0.6773265 ]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32248092 0.677519  ]\n",
            " [0.32267347 0.6773265 ]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32248092 0.677519  ]\n",
            " [0.32267347 0.6773265 ]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32293305 0.6770669 ]\n",
            " [0.32730445 0.6726955 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.3261829  0.6738171 ]\n",
            " [0.35911217 0.64088786]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.35756814 0.64243186]\n",
            " [0.57354707 0.42645293]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.57458866 0.42541134]\n",
            " [0.32730445 0.6726955 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32804754 0.6719524 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37397137 0.6260286 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64327985 0.35672024]\n",
            " [0.57354707 0.42645293]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.5727641  0.4272359 ]\n",
            " [0.32267347 0.6773265 ]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32293305 0.6770669 ]\n",
            " [0.32730445 0.6726955 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.3261829  0.6738171 ]\n",
            " [0.35911217 0.64088786]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.36098886 0.63901114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.5864123  0.41358766]\n",
            " [0.35911217 0.64088786]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.36098886 0.63901114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.59231764 0.4076824 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37749612 0.6225039 ]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6585519  0.34144813]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6563695  0.3436305 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64327985 0.35672024]\n",
            " [0.57354707 0.42645293]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.5727641  0.4272359 ]\n",
            " [0.32267347 0.6773265 ]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32248092 0.677519  ]\n",
            " [0.32267347 0.6773265 ]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32293305 0.6770669 ]\n",
            " [0.32730445 0.6726955 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32804754 0.6719524 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37749612 0.6225039 ]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6563695  0.3436305 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64649886 0.35350114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.5864123  0.41358766]\n",
            " [0.35911217 0.64088786]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.35756814 0.64243186]\n",
            " [0.57354707 0.42645293]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.5727641  0.4272359 ]\n",
            " [0.32267347 0.6773265 ]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32248092 0.677519  ]\n",
            " [0.32267347 0.6773265 ]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32248092 0.677519  ]\n",
            " [0.32267347 0.6773265 ]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32248092 0.677519  ]\n",
            " [0.32267347 0.6773265 ]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32293305 0.6770669 ]\n",
            " [0.32730445 0.6726955 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32804754 0.6719524 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37397137 0.6260286 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64649886 0.35350114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.59231764 0.4076824 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37749612 0.6225039 ]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6585519  0.34144813]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6585519  0.34144813]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6563695  0.3436305 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64649886 0.35350114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.59231764 0.4076824 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37749612 0.6225039 ]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6585519  0.34144813]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6585519  0.34144813]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6585519  0.34144813]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6585519  0.34144813]\n",
            " [0.65781087 0.34218907]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.6563695  0.3436305 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64327985 0.35672024]\n",
            " [0.57354707 0.42645293]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.57458866 0.42541134]\n",
            " [0.32730445 0.6726955 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32804754 0.6719524 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37397137 0.6260286 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64327985 0.35672024]\n",
            " [0.57354707 0.42645293]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.57458866 0.42541134]\n",
            " [0.32730445 0.6726955 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.32804754 0.6719524 ]\n",
            " [0.37626716 0.6237328 ]\n",
            " [0.6533428  0.34665722]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.37397137 0.6260286 ]\n",
            " [0.6447714  0.35522854]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.64327985 0.35672024]\n",
            " [0.57354707 0.42645293]\n",
            " [0.32465208 0.67534786]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.57458866 0.42541134]\n",
            " [0.32730445 0.6726955 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.3261829  0.6738171 ]\n",
            " [0.35911217 0.64088786]\n",
            " [0.58110833 0.4188917 ]\n",
            " [0.34444442 0.65555555]\n",
            " [1.         1.        ]] \n",
            "\n",
            "[[0.36098886 0.63901114]\n",
            " [0.5900324  0.4099676 ]\n",
            " [0.36952144 0.6304786 ]\n",
            " [0.6272727  0.37272727]\n",
            " [1.         1.        ]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "alpha, beta, gamma, _ = hmm_forwards_backwards_jax(hmm, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQH4guSkzq4B",
        "outputId": "106739fb-d01e-4c5f-8901-24ab74d3e504"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 710 ms, sys: 8.28 ms, total: 718 ms\n",
            "Wall time: 735 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c5f9Z7Zpz5tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = PRNGKey(0)\n",
        "num_states = 3\n",
        "num_obs = 5\n",
        "num_timesteps = 15\n",
        "\n",
        "data = jax.random.choice(key, num_obs, (num_timesteps,))\n",
        "key, _ = split(key)\n",
        "\n",
        "transmat = jax.random.uniform(key, shape=(num_states, num_states))\n",
        "transmat, _ = normalize(transmat, axis=1)\n",
        "print(transmat)\n",
        "key, _ = split(key)\n",
        "\n",
        "obsmat = jax.random.uniform(key, shape=(num_states, num_obs))\n",
        "obsmat, _ = normalize(obsmat, axis=1)\n",
        "print(obsmat)\n",
        "key, _ = split(key)\n",
        "\n",
        "prior = jnp.array([0.33, 0.33, 0.34])"
      ],
      "metadata": {
        "id": "7W7oRPpIz5-R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}